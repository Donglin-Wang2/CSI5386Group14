{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_A1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "McPXJyhG_YXT"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OH7RuzAA9n7E"
      },
      "source": [
        "reddit_sarcasm_f = open('reddit_sarcasm.txt')\n",
        "reddit_sarcasm = reddit_sarcasm_f.readlines()\n",
        "\n",
        "\n",
        "stop_words_f = open('stop_words.txt')\n",
        "stop_word_lines = stop_words_f.readlines()\n",
        "stop_words_tokenizer = Tokenizer()\n",
        "stop_words_tokenizer.fit_on_texts(stop_word_lines)\n",
        "stop_words = list(stop_words_tokenizer.word_counts)\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cASuHHSATdy",
        "outputId": "f38c9198-ad75-44fd-ff8b-567d9f4b4e6d"
      },
      "source": [
        "tokenizer = Tokenizer(filters='')\n",
        "\n",
        "tokenizer.fit_on_texts(reddit_sarcasm)\n",
        "\n",
        "n_tokens = 0\n",
        "n_words_once = 0\n",
        "\n",
        "for word, count in tokenizer.word_counts.items():\n",
        "  n_tokens += count\n",
        "\n",
        "  if (count == 1) and word not in '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n':\n",
        "    n_words_once += 1\n",
        "\n",
        "n_types = len(tokenizer.word_index)\n",
        "\n",
        "ratio = n_types / n_tokens\n",
        "\n",
        "word_counts_sorted = dict(sorted(tokenizer.word_counts.items(), key=lambda x: x[1], reverse=True))\n",
        "\n",
        "freq_20 = list(word_counts_sorted.items())[:20]\n",
        "\n",
        "print(f'# of tokens: { n_tokens }')\n",
        "print(f'# of types: { n_types }')\n",
        "print(f'type/token ratio: { ratio }')\n",
        "print(f'# of words appearing once: {n_words_once}')\n",
        "print(f'20 most frequent: {freq_20}')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# of tokens: 2419971\n",
            "# of types: 189969\n",
            "type/token ratio: 0.07850052748565996\n",
            "# of words appearing once: 121787\n",
            "20 most frequent: [('the', 91217), ('to', 60412), ('a', 57180), ('and', 47915), ('i', 42662), ('of', 38683), ('is', 32297), ('in', 30320), ('that', 26669), ('you', 26448), ('for', 23284), ('it', 22554), ('on', 16974), ('this', 15850), ('with', 15162), ('be', 15035), ('have', 14752), ('but', 13842), ('are', 13526), ('was', 13464)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1DWBnUsJHFE",
        "outputId": "cfc9cffc-804a-4821-8f45-08faeed23af2"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(reddit_sarcasm)\n",
        "\n",
        "n_tokens = 0\n",
        "\n",
        "for word, count in tokenizer.word_counts.items():\n",
        "  n_tokens += count\n",
        "\n",
        "n_types = len(tokenizer.word_index)\n",
        "\n",
        "ratio = n_types / n_tokens\n",
        "\n",
        "word_counts_sorted = dict(sorted(tokenizer.word_counts.items(), key=lambda x: x[1], reverse=True))\n",
        "freq_20 = list(word_counts_sorted.items())[:20]\n",
        "\n",
        "print(f'# of tokens: { n_tokens }')\n",
        "print(f'# of types: { n_types }')\n",
        "print(f'lexical diversity: { ratio }')\n",
        "print(f'20 most frequent: {freq_20}')\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# of tokens: 2431732\n",
            "# of types: 74793\n",
            "lexical diversity ratio: 0.030757090008273938\n",
            "20 most frequent: [('the', 93041), ('to', 61223), ('a', 58054), ('and', 48884), ('i', 47204), ('of', 39028), ('is', 33714), ('in', 31506), ('that', 29201), ('you', 29078), ('it', 28495), ('for', 24006), ('this', 18587), ('on', 17899), ('with', 15669), ('be', 15555), ('have', 15066), ('but', 14450), ('are', 14096), ('was', 13789)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybbb2arURTPK",
        "outputId": "85bb16dd-1252-4d2e-da5d-1eba426878aa"
      },
      "source": [
        "tokens_sorted_no_stop = word_counts_sorted\n",
        "\n",
        "for x in stop_words:\n",
        "  if x in tokens_sorted_no_stop:\n",
        "    del tokens_sorted_no_stop[x]\n",
        "\n",
        "freq_20 = list(tokens_sorted_no_stop.items())[:20]\n",
        "\n",
        "\n",
        "n_tokens = 0\n",
        "\n",
        "for word, count in tokens_sorted_no_stop .items():\n",
        "  n_tokens += count\n",
        "\n",
        "n_types = len(tokens_sorted_no_stop)\n",
        "\n",
        "ratio = n_types / n_tokens\n",
        "\n",
        "print(f'# of tokens: { n_tokens }')\n",
        "print(f'# of types: { n_types }')\n",
        "print(f'lexical density: { ratio }')\n",
        "print(f'20 most frequent: {freq_20}')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# of tokens: 1057307\n",
            "# of types: 74088\n",
            "lexical density: 0.07007236308848802\n",
            "20 most frequent: [(\"it's\", 8958), ('people', 8170), (\"don't\", 7512), (\"i'm\", 7039), ('think', 5718), ('know', 5033), ('game', 4927), ('2', 3880), ('fuck', 3784), ('trump', 3592), ('right', 3236), ('1', 3157), (\"that's\", 3132), ('3', 2820), ('back', 2812), (\"can't\", 2771), (\"you're\", 2523), (\"i've\", 2506), (\"doesn't\", 2412), ('play', 2283)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGmeXuE4aU95"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}