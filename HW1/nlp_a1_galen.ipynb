{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_A1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "McPXJyhG_YXT"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OH7RuzAA9n7E"
      },
      "source": [
        "reddit_sarcasm_f = open('reddit_sarcasm.txt')\n",
        "reddit_sarcasm = reddit_sarcasm_f.readlines()\n",
        "\n",
        "stop_words_f = open('stop_words.txt')\n",
        "stop_word_lines = stop_words_f.readlines()\n",
        "stop_words_tokenizer = Tokenizer()\n",
        "stop_words_tokenizer.fit_on_texts(stop_word_lines)\n",
        "stop_words = list(stop_words_tokenizer.word_counts)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cASuHHSATdy",
        "outputId": "f38c9198-ad75-44fd-ff8b-567d9f4b4e6d"
      },
      "source": [
        "tokenizer = Tokenizer(filters='')\n",
        "tokenizer.fit_on_texts(reddit_sarcasm)\n",
        "raw_dict = tokenizer.word_counts.copy()\n",
        "no_punc_dict = tokenizer.word_counts.copy()\n",
        "no_stop_dict = tokenizer.word_counts.copy()\n",
        "no_stop_punc_dict = tokenizer.word_counts.copy()\n",
        "n_once = 0\n",
        "\n",
        "for word, count in tokenizer.word_counts.items():\n",
        "\n",
        "  if word in '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n':\n",
        "    if count == 1:\n",
        "      n_once += 1\n",
        "    del no_punc_dict[word]\n",
        "    del no_stop_punc_dict[word]\n",
        "\n",
        "  if word in stop_words:\n",
        "    del no_stop_dict[word]\n",
        "    del no_stop_punc_dict[word]\n",
        "\n",
        "n_tokens_type = len(raw_dict)\n",
        "n_tokens_type_no_punc = len(no_punc_dict)\n",
        "n_tokens_type_no_stop = len(no_stop_dict)\n",
        "n_tokens_type_no_punc_stop = len(no_stop_punc_dict)\n",
        "n_tokens = sum(count for count in raw_dict.values())\n",
        "n_tokens_no_stop = sum(count for count in no_stop_dict.values())\n",
        "n_tokens_no_punc = sum(count for count in no_punc_dict.values())\n",
        "n_tokens_no_stop_punc = sum(count for count in no_stop_punc_dict.values())\n",
        "raw_sorted = dict(sorted(raw_dict.items(), key=lambda x: x[1], reverse=True))\n",
        "no_punc_sorted = dict(sorted(no_punc_dict.items(), key=lambda x: x[1], reverse=True))\n",
        "no_stop_sorted = dict(sorted(no_stop_dict.items(), key=lambda x: x[1], reverse=True))\n",
        "no_punc_stop_sorted = dict(sorted(no_stop_punc_dict.items(), key=lambda x: x[1], reverse=True))\n",
        "\n",
        "print(f'# of tokens: { n_tokens }')\n",
        "print(f'# of types: { n_tokens_type }')\n",
        "print(f'type/token ratio: { n_tokens / n_tokens_type }')\n",
        "print(f'# of words appearing once: {n_once}')\n",
        "print(f'# of words (excluding punctuation): { n_tokens_no_punc }')\n",
        "print(f'type/token ratio (excluding punctuation): { n_tokens_no_punc / n_tokens_type_no_punc }')\n",
        "print(f'Top 3 most freqeuent words and their frequencies: { list(raw_sorted.items())[:3] }')\n",
        "print(f'type/token ratio (excluding punctuation and stopwords): { n_tokens_type_no_punc_stop/ n_tokens_no_stop_punc }')\n",
        "print(f'Top 3 most frequent words and their frequencier (excluding stopwords): { list(no_stop_sorted.items())[:3] }')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of tokens: 2419971\n# of types: 189969\ntype/token ratio: 12.738767904237006\n# of words appearing once: 0\n# of words (excluding punctuation): 2410833\ntype/token ratio (excluding punctuation): 12.692469280096029\nTop 3 most freqeuent words and their frequencies: [('the', 91217), ('to', 60412), ('a', 57180)]\ntype/token ratio (excluding punctuation and stopwords): 0.16417371811665993\nTop 3 most frequent words and their frequencier (excluding stopwords): [(\"it's\", 8259), (\"don't\", 7132), ('people', 7061)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Yeah, I get that argument. At this point, I'd prefer is she lived in NC as well.\"\n\n[('\"yeah,', 1), ('i', 1), ('get', 1), ('that', 1), ('argument.', 1), ('at', 1), ('this', 1), ('point,', 1), (\"i'd\", 1), ('prefer', 1), ('is', 1), ('she', 1), ('lived', 1), ('in', 1), ('nc', 1), ('as', 1), ('well.\"\\n', 1)]\n"
          ]
        }
      ],
      "source": [
        "# calculating bigram stats\n",
        "sentence_tokenizer = Tokenizer(filters='')\n",
        "for sentence in reddit_sarcasm[:1]:\n",
        "    print(sentence)\n",
        "    sentence_tokenizer.fit_on_texts([sentence])\n",
        "    print(list(sentence_tokenizer.word_counts.items()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}