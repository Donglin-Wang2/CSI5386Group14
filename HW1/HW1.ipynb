{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "from spacy.pipeline import Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "with open('reddit_sarcasm.txt') as file:\n",
    "        sentences = file.readlines()\n",
    "print(len(sentences))\n",
    "model = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = model(''.join(sentences[:100]))\n",
    "for v in model.vocab:\n",
    "    v.is_stop = False\n",
    "stop_words_set = set()\n",
    "with open('StopWords.txt') as file:\n",
    "    stop_words = file.readlines()\n",
    "    for stop_word in stop_words:\n",
    "        stop_words_set.add(stop_word[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Getting general information about the corpus\n",
    "pos_stat = {}\n",
    "pos_no_punc = {}\n",
    "pos_no_stop = {}\n",
    "pos_no_punc_stop = {}                \n",
    "word_stat = {}             \n",
    "word_no_punc = {}\n",
    "word_no_stop = {}\n",
    "word_no_punc_stop = {}\n",
    "hehe = None\n",
    "for token in doc:\n",
    "    if not token.lemma_ in stop_words_set:\n",
    "        word_no_stop[token.text] = word_no_stop.get(token.text, 0) + 1\n",
    "        pos_no_stop[token.pos_] = pos_no_stop.get(token.pos_, 0) + 1\n",
    "        if token.pos_ != \"PUNCT\":\n",
    "            word_no_punc_stop[token.text] = word_no_punc_stop.get(token.text, 0) + 1\n",
    "            pos_no_punc_stop[token.pos_] = pos_no_punc_stop.get(token.pos_, 0) + 1\n",
    "    if token.pos_ != 'PUNCT':\n",
    "        word_no_punc[token.text] = word_no_punc.get(token.text, 0) + 1\n",
    "        pos_no_punc[token.pos_] = pos_no_punc.get(token.pos_, 0) + 1\n",
    "    word_stat[token.text] = word_stat.get(token.text, 0) + 1\n",
    "    pos_stat[token.pos_] = pos_stat.get(token.pos_, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "# of tokens: \n1270\n# of types: \n18\nType/token ratio: \n0.013492063492063493\nTokens appeared only once: \n911\n# of words (excluding punctuation): \n1260\nList the top 3 most frequent words and their frequencies: \nYeah,argument,At,\nType/token ratio (excluding punctuation and stopwords): \n0.015337423312883436\nList the top 3 most frequent words and their frequencies (excluding stopwords)\nYeah,argument,prefer,"
     ]
    }
   ],
   "source": [
    "print(\"# of tokens: \")\n",
    "print(len(word_stat))\n",
    "print(\"# of types: \")\n",
    "print(len(pos_stat))\n",
    "print(\"Type/token ratio: \")\n",
    "print(len(pos_no_punc) / len(word_no_punc))\n",
    "print(\"Tokens appeared only once: \")\n",
    "num_singleton = 0\n",
    "for k, v in word_stat.items():\n",
    "    if v == 1:\n",
    "        num_singleton += 1\n",
    "print(num_singleton)\n",
    "print(\"# of words (excluding punctuation): \")\n",
    "print(len(word_no_punc))\n",
    "print('List the top 3 most frequent words and their frequencies: ')\n",
    "sorted_word_stat = sorted(word_stat.items(), key=lambda x:x[1])\n",
    "for k, v in sorted_word_stat[:3]:\n",
    "    print(k, end=',')\n",
    "print()\n",
    "print('Type/token ratio (excluding punctuation and stopwords): ')\n",
    "print(len(pos_no_punc_stop) / len(word_no_punc_stop))\n",
    "print('List the top 3 most frequent words and their frequencies (excluding stopwords)')\n",
    "sorted_word_no_stop = sorted(word_no_stop.items(), key=lambda x:x[1])\n",
    "for k, v in sorted_word_no_stop[:3]:\n",
    "    print(k, end=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}