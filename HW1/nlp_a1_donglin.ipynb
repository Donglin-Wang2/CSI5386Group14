{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_A1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "McPXJyhG_YXT"
      },
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize, RegexpTokenizer"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OH7RuzAA9n7E"
      },
      "source": [
        "with open('./reddit_sarcasm.txt') as reddit_file:\n",
        "  content = reddit_file.read()\n",
        "with open('./stop_words.txt') as stopword_file:\n",
        "  stop_words = stopword_file.readlines()\n",
        "\n",
        "raw_tokens = [ele.lower() for ele in word_tokenize(content)]\n",
        "stop_words = set([word[:-1] for word in stop_words])"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cASuHHSATdy",
        "outputId": "f38c9198-ad75-44fd-ff8b-567d9f4b4e6d"
      },
      "source": [
        "raw_dict = dict(Counter(raw_tokens))\n",
        "no_punc_dict = raw_dict.copy()\n",
        "no_stop_dict = raw_dict.copy()\n",
        "no_stop_punc_dict = raw_dict.copy()\n",
        "n_once = 0\n",
        "\n",
        "for word, count in raw_dict.items():\n",
        "\n",
        "  if count == 1: n_once += 1\n",
        "\n",
        "  if re.match('\\w+', word) == None:\n",
        "    del no_punc_dict[word]\n",
        "    del no_stop_punc_dict[word]\n",
        "\n",
        "  if word in stop_words:\n",
        "    del no_stop_dict[word]\n",
        "    del no_stop_punc_dict[word]\n",
        "\n",
        "n_tokens_type = len(raw_dict)\n",
        "n_tokens_type_no_punc = len(no_punc_dict)\n",
        "n_tokens_type_no_stop = len(no_stop_dict)\n",
        "n_tokens_type_no_punc_stop = len(no_stop_punc_dict)\n",
        "n_tokens = sum(count for count in raw_dict.values())\n",
        "n_tokens_no_stop = sum(count for count in no_stop_dict.values())\n",
        "n_tokens_no_punc = sum(count for count in no_punc_dict.values())\n",
        "n_tokens_no_stop_punc = sum(count for count in no_stop_punc_dict.values())\n",
        "raw_sorted = dict(sorted(raw_dict.items(), key=lambda x: x[1], reverse=True))\n",
        "no_punc_sorted = dict(sorted(no_punc_dict.items(), key=lambda x: x[1], reverse=True))\n",
        "no_stop_sorted = dict(sorted(no_stop_dict.items(), key=lambda x: x[1], reverse=True))\n",
        "no_punc_stop_sorted = dict(sorted(no_stop_punc_dict.items(), key=lambda x: x[1], reverse=True))\n",
        "\n",
        "print(f'# of tokens: { n_tokens }')\n",
        "print(f'# of types: { n_tokens_type }')\n",
        "print(f'type/token ratio: { n_tokens_type / n_tokens }')\n",
        "print(f'# of words appearing once: {n_once}')\n",
        "print(f'# of words (excluding punctuation): { n_tokens_no_punc }')\n",
        "print(f'type/token ratio (excluding punctuation): { n_tokens_type_no_punc / n_tokens_no_punc }')\n",
        "print(f'Top 3 most frequent words and their frequencies: { list(raw_sorted.items())[:3] }')\n",
        "print(f'type/token ratio (excluding punctuation and stopwords): { n_tokens_type_no_punc_stop/ n_tokens_no_stop_punc }')\n",
        "print(f'Top 3 most frequent words and their frequencies (excluding stopwords): { list(no_punc_stop_sorted.items())[:3] }')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of tokens: 2915727\n# of types: 83226\ntype/token ratio: 0.028543824576169167\n# of words appearing once: 44380\n# of words (excluding punctuation): 2431544\ntype/token ratio (excluding punctuation): 0.033083917050236396\nTop 3 most frequent words and their frequencies: [('.', 122712), ('the', 92876), (\"''\", 85048)]\ntype/token ratio (excluding punctuation and stopwords): 0.07924873707884822\nTop 3 most frequent words and their frequencies (excluding stopwords): [(\"n't\", 22721), ('people', 8276), ('think', 5718)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(('it', 's'), 8979), (('don', 't'), 7518), (('in', 'the'), 7386)]\n"
          ]
        }
      ],
      "source": [
        "with open('./reddit_sarcasm.txt') as reddit_file:\n",
        "  sentences = reddit_file.readlines()\n",
        "no_punc_tokenizer = RegexpTokenizer(r'\\w+')\n",
        "bigrams = []\n",
        "for sentence in sentences:\n",
        "    filtered_sent = []\n",
        "    words = no_punc_tokenizer.tokenize(sentence)\n",
        "    words = [ele.lower() for ele in words]\n",
        "    for word in words:\n",
        "        if word not in stop_words:\n",
        "            filtered_sent.append(word)\n",
        "    bigrams += list(zip(words, words[1:]))\n",
        "bigram_counts = dict(Counter(bigrams))\n",
        "bigram_counts_sorted = dict(sorted(bigram_counts.items(), key=lambda x:x[1], reverse=True))\n",
        "print(f'Top 3 most frequent bigrams and their frequencies: {list(bigram_counts_sorted.items())[:3]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}